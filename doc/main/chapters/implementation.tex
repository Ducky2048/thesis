\chapter{Implementation}
\section{Choices in Technologies}
\label{sec:techchoices}
In this chapter, we outline the technologies we choose, and the reasoning for choosing them.

\subsection{Backend}
\label{subsec:techbackend}
The backend is divided into two parts, the signing service and the verification service.
They are developed independently in different programming languages.

We chose to split the backend into two because signature verification has to be available both online and offline.

Using two different programming languages and sets of libraries,
implementing common formats and protocols independently from specification alone,
allows us to hedge against the risk of some flaws in the libraries: if a library used in the signing service produced flawed output,
it would likely be discovered by the verification service since it uses a different implementation of the same concepts.
The exact same flaw being present in two different libraries implemented in different programming languages is rather small.

On top of that, we can test ourselves how well we've specified the protocols and formats:
if these two services can be used together without problems, the specification was precise enough.
If not, we learn where we must improve it, which is a win-win situation.

\subsubsection{Signing Service}
The signing service is implemented in the Kotlin programming language~\cite{kotlin} using the Ktor framework~\cite{ktor}.
For the signing service we chose Kotlin because it is a modern and concise programming language,
providing null safety, coroutines for asynchronous programming,
support for functional programming with higher-order functions and closures,
and it's genuinely nice to program in.
Kotlin offers seamless Java interoperability, allowing us to make use of the excellent and extensive Java ecosystem.

We haven't picked Kotlin and Ktor at random, we've discussed which language and framework to use for the signing server at length.
The following table lists a short overview which other choices we've discussed but chosen not to pursue.

\begin{longtable}{p{1.5cm}p{2cm}p{11cm}}
    \hline
    \textbf{Language} & \textbf{Framework} & \textbf{Reason for non-consideration} \\ \hline
    Java & Spring MVC & Steep learning curve, vast amount of functionality to learn, too large for our needs, little previous knowledge on our part \\ \hline
    Java & Vaadin & Generated frontend is slow to use because of network latency, too much uncertainty with regards to \gls{WASM} integration:  Is it possible? How? How long will it take us, how much will we have to implement ourselves? \\ \hline
    C\# & .NET Core & Zero previous knowledge on our part both for the language and the framework, aversion to Microsoft because of their attempts to undermine open-source software~\cite{mseee} \\ \hline
    Scala & Play & Little previous knowledge of the language, uncertainty of the time required to learn it properly, but otherwise an interesting contender \\ \hline
    Python & Django & Huge framework to learn, no type safety, slow runtime performance \\ \hline
    \caption{Programming languages and frameworks considered for use in the signing service implementation}
\end{longtable}


\subsubsection{Verifier}
The verification service is implemented in the Go programming language~\cite{golang}.
The verification service being separate allows us to distribute a smaller program,
and since Go binaries are always statically linked we don't have to worry about shipping dependencies.
TODO more reasoning about not using generics.


\subsection{Frontend}
\label{subsec:techfrontend}

Given that the frontend must support the three desktop operating systems Microsoft Windows,
GNU/Linux as well as Apple MacOS,
the technological choices available to us are limited.
On the desktop, we could use the \gls{JVM} platform and the JavaFX \gls{GUI} library, whereas on the phones
we could use Flutter~\cite{flutterframework}.
However, developing three applications on five platforms using two new-to-us frameworks and programming languages
would take a lot more time and resources than what is available to us in the scope of this thesis.

In order to reduce complexity and enable code reuse, we decide to implement the frontend as a web application.
Web frontents are capable of running in any modern web browser regardless of platform, be it mobile or desktop.
We're not happy about this, as we would much rather use mature, strongly-typed and well-designed languages and frameworks,
but we're forced to make this compromise in order to meet our objectives in the time available.
In order to reduce the pain, we will use TypeScript, which is a typed superset of JavaScript~\cite{loltypes}.
We discussed implementing a \gls{SPA} using Angular~\cite{angular},
but for our use case it's overkill as there isn't too much client-side logic.
In the interest of a small, fast-loading site we chose to stick with plain \gls{HTML} and \gls{CSS}
and only adding as much JavaScript as necessary.

\subsection{Client-Side File Hashing in the Web Browser}
\label{subsec:browserhashing}
However, the decision to implement the frontend as a web application presents us with a challenge:
hashing the files to be signed client-side in the web browser itself.
If we had implemented "proper" client applications this would've been easy, but in a web browser and using its
JavaScript language not so much: it simply wasn't designed with performance in mind.

The easiest solution would be to upload the files to be signed to the server and hash them there,
but this would be a clear violation of the least-information principle (the server doesn't need the file, only the hash)
and a breach of user privacy.
Nevermind the fact that signing large files could take a very long time over slow network connections,
and turn out to be quite expensive for mobile users billed for data by volume.

Another solution would be to ask the user to enter the file hashes instead of selecting files,
but this would be very user-unfriendly and most likely too much to ask from many users.

It is clear we must find a way to hash files in the web browser itself.
In order to achieve this we have found the following options:

\begin{enumerate}
    \item Using the browser-implemented \texttt{SubtleCrypto}~\cite{subtlecrypto} \gls{API}
    \item Using the \texttt{CryptoJS}~\cite{cryptojs} JavaScript implementation
    \item Using a \gls{WASM}-based implementation
\end{enumerate}

Each of these options comes with a number of advantages and disadvantages, as discussed in more detail in the following sections.

\subsubsection{Using SubtleCrypto}
\label{subsec:subtlecrypto}
The \texttt{SubtleCrypto} class offers the \texttt{digest(algorithm, data)} method~\cite{subtlecrypto}, which can be used to
calculate \gls{SHA-256} checksums.
The advantage of using this implementation is that it is available in all modern browsers\footnote{Where modern browsers means Mozilla Firefox, Google Chrome/Chromium, and Microsoft Edge, not older than the respective versions available in 2018},
and since it's executed with native code, being able to take advantage of \gls{AVX2} instructions, instead of JavaScript it should be quite fast.
There's a major drawback though: hashing a large amount of data progressively is not supported, the data has to be
passed to the function en bloc, as seen in listing~\ref{lst:subtlecrypto}.

\lstinputlisting[caption={Using SubtleCrypto for calculating SHA-256 checksums}, captionpos=b, language=JavaScript, label={lst:subtlecrypto}]{listings/subtlecrypto.js}

Our testing showed that selecting files larger than 200MB crashes Firefox tabs when trying to read their contents
into memory before we could pass it to the \texttt{digest} function.
If we assume the users will only ever select small files this should not pose a problem, but unfortunately it's not safe to assume this.
Furthermore, this limit is probably lower still on mobile devices such as smartphones (although we didn't test this).

\subsubsection{Using CryptoJS}
\label{subsec:cryptojs}
\texttt{CryptoJS} does not have the limitation of \texttt{SubtleCrypto} and supports progressive hashing\footnote{
By progressive hashing we mean the ability to pass to the hash function the data piece by piece in order to avoid holding all of it in memory at once.},
as seen in listing~\ref{lst:cryptojsprogressive}.

\lstinputlisting[caption={Progressive SHA-256 hashing using CryptoJS},captionpos=b,language=JavaScript,label={lst:cryptojsprogressive}]{listings/cryptojs.js}

The advantage of using \texttt{CryptoJS} over \texttt{SubtleCrypto} is, as mentioned, the ability to hash piece-wise.

The disadvantage is that we need to load a third-party JavaScript library, using built-in functionality would be preferable.

And since JavaScript is an interpreted language, using it to calculate the checksums results in performance figures everyone but web developers would laugh at.
This is a problem especially on mobile devices limited in compute and memory resources as well as battery capacity.
Since we want to support mobile devices properly, and don't want to limit users to small files, we must do better.

\subsubsection{Using a WASM-based implementation}
\label{subsec:wasmhashing}
\gls{WASM} provides a low-level virtual machine in the web browser itself,
running machine-independent binary code, comparable to the \gls{JVM} or the \gls{CLR},
albeit much simpler and much less sophisticated.
By using this virtual machine we should be able to run code at near-native speed written in a statically-typed, compiled language such as Rust, C/C++ or Go.
Thus we expect significant performance gains over a JavaScript-based implementation.
While developing the \gls{WASM}-based hashing programmes, we encountered some interesting challenges, as described in the following paragraphs.

\paragraph{CORS Policy} While JavaScript can be executed simply by pointing the browser at a local \gls{HTML} file, the same doesn't work for \gls{WASM}.
The browser's security policy forbids it due to its \gls{CORS} rule~\cite{cors}.
We solved this by starting the \gls{HTTP} server built in to Go's standard library and having the browser load the \gls{WASM} binary through \gls{HTTP}.
The code is in appendix~\ref{chap:appendix_golangwebserver}.
For the Rust-based implementation we used the built-in web server of webpack~\cite{webpack}.

\paragraph{JavaScript/WASM Compatibility} The Golang project conveniently provides a file containing the necessary boilerplate code to load, start and interact with \gls{WASM} programmes called \texttt{wasm\_exec.js}.
But there's a catch: for each version of Go, the version of the accompanying \texttt{wasm\_exec.js} file used must match precisely.
If it doesn't, the code will crash with a segmentation fault.
It took us quite some time to figure out why the code we'd written only a few days prior would segfault now with no changes made to it.

\paragraph{Passing data} Functions written in Go intended to be used from the JavaScript side of things need to have a very specific signature.
As can be seen in listing~\ref{lst:funcsignaturewasm}, there is no typing: all arguments passed to the function are of type \texttt{js.Value} and the return value must be of type \texttt{interface\{\}}\footnote{\texttt{interface\{\}} is Go's equivalent of Java's \texttt{Object}, it could be anything.}.
This posed us with the challenge of detecting the types and casting the data passed accordingly.

\lstinputlisting[caption={Golang WASM function signature}, captionpos=b, language=Go, label={lst:funcsignaturewasm}]{listings/wasmfunc.go}

We've worked on this for hours, producing ugly reflection-based hacks, until we decided to just agree on the types of the arguments and return values beforehand despite the open function signature.
Now all that's needed is a little boilerplate to convert a JavaScript \texttt{Uint8Array} to a Golang \texttt{[]byte}, as seen in listing~\ref{lst:jscastingtogo}.

\lstinputlisting[caption={Uint8Array to {[]}byte}, captionpos=b, language=Go, label={lst:jscastingtogo}, captionpos=b]{listings/jscasting.go}

\paragraph{Goroutines} Go features its own concurrency primitive called Goroutines.
From a programmers' perspective, they can be used like threads, but they carry much less overhead.
Communication between goroutines is achieved by using so-called channels, which on a high level are comparable to queues.
Unfortunately, the \gls{WASM} specification wasn't drafted with this kind of concurrency in mind.
Go is forced to unwind and restore the call stack when switching between goroutines, which is very expensive~\cite{lolnogoroutines}.
We rewrote the Go programme to work without them, and we've seen a small but significant performance improvement.

\paragraph{Rust based WASM}
As the Go implementation also includes the Go runtime,
which makes the wasm file much larger,
and starts a programme that will run continuously in the background it isn't the optimal choice for creating a WebAssembly implementation.
As neither of us knows any other of the other languages that compile to WebAssembly well, we excluded them at first.
However with the drawbacks of the Go based implementation we decided to try to implement a Rust-based version as well,
in order to see how they compare both in performance and ease of development.


\subsubsection{Performance Comparison}
\label{subsec:perfcomphashing}
No one likes waiting for slow software to do its work, and neither do we.
This is why we decided to compare the performance of the aforementioned options in a simple test:
we measure the time it takes for the browser to calculate the checksum of 1GB of random data using the aforementioned methods.
The code used for each example is in appendix~\ref{ch:appendix-in-browser-hashing-code}.
The tests were run on Debian 10 using Firefox 69 on an Intel i7-8550U.
The results can be seen in figure~\ref{fig:hashingperformance}.

As expected, the in-browser \texttt{SubtleCrypto}-implementation is the fastest, followed by the Rust-based implementation.
JavaScript is so ridiculously slow it's not even trying to compete.
In order to provide a reference to compare the hashing speeds to we include the performance of the \texttt{openssl} command-line programme.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{images/hashingperformance.png}
        \caption{Hashing speed in MB/s (higher is better)}
        \label{fig:hashingperformance}
    \end{center}
\end{figure}


\subsection{Deciding On The In-Browser Hashing Implementation}
\label{subsec:deciding-on-the-in-browser-hashing-implementation}
It is clear from figure~\ref{fig:hashingperformance} that \texttt{SubtleCrypto} is the fastest of the options we tried.
Unfortunately, since it doesn't support piece-wise hashing we're forced to pick the next-fastest option,
the Rust-based implementation running in the \gls{WASM} \gls{VM}.
Using Rust provides us with another significant advantage:
the toolchain is highly developed.
Upon compilation, the toolchain auto-generates TypeScript declaration files containing the function signatures
the \gls{WASM} module exposes to the JavaScript world.
This is very nice, since it allows for compile-time type checking and for smarter code completion in the \gls{IDE},
as shown in figure~\ref{fig:dtside}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{images/dtside.png}
        \caption{Code Completion in Intellij}
        \label{fig:dtside}
    \end{center}
\end{figure}


\section{Implementation Components}\label{sec:implementation-components}
\subsection{Design Principles}\label{subsec:design-principles}
We've split the implementation of the Signing Service and the Verifier into small,
replaceable modules with clearly defined interfaces.
We've done this to achieve clear separation of concern and loose coupling,
in the spirit of the Law of Demeter~\cite{demeter}.

\paragraph{The Law of Demeter} is a well-known heuristic that states that any module should not know about the
internals of the objects it manipulates.
It is a special case of Loose Coupling.
Objects should hide their data, and expose operations.
This makes it easy to add new types of objects without requiring changing existing behaviours.
It also makes it hard to add new behaviours to existing objects.
Data structures should expose data and have no significant behaviour.

\paragraph{Loose Coupling} refers to the degree of knowledge that one component has of another.
Structuring programs to consist of components that know little of one another
results in easy-to-understand, easy-to-test code.
Developers new to the project can start with work on a small module and don't need to understand the whole system.
Refactoring (or replacing) the implementation of a component becomes easy,
as there are clear boundaries,
and changing the implementation of one component does not affect the others as long as
the boundary contract remains satisfied.


\paragraph{Inversion of Control} enables us to remove the few interdependencies remaining in the modules:
knowing about each other.
A component should not care about where, how and why another component is implemented,
it should only care about being provided the behaviour it requires.
Inversion of Control through Dependency Injection allows each component to declare
its dependencies through interfaces,
and having it supplied the implementation of that interface from the system it is part of,
without knowing anything whatsoever about that implementation.

Applying these methodologies doesn't guarantee clean code, nothing does.
However in our experience, modularisation, decoupling, and separation of concerns is the best single principle
to apply to software development to achieve some level of code quality.

\subsection{Components of the Signing Service}\label{subsec:modules-of-the-signing-service}
There are two groups of components in the signing service:
\begin{itemize}
    \item The views: They implement the \gls{REST} interface to the outside world.
    \item The services: Replaceable components providing functionality needed by the views in order to
    be able to meet their purpose.
\end{itemize}

For a \gls{UML} component diagram showing a simplified overview of the components, see figure~\ref{fig:signingservicecomponents}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{images/signing_service_components.png}
        \caption{Signing Service Components}
        \label{fig:signingservicecomponents}
    \end{center}
\end{figure}


\subsection{Components of the Verifier}\label{subsec:modules-of-the-verifier}
The verifier has an embedded webserver with a single \gls{API} endpoint.
The webserver handles the \gls{HTTP} side of things.
It interfaces with the verifier modules, which are split by responsability (e.g. \texttt{id\_token} verification).
For a \gls{UML} component diagram showing a simplified overview of the components, see figure~\ref{fig:verifiercomponents}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{images/verifier_components.png}
        \caption{Verifier Components}
        \label{fig:verifiercomponents}
    \end{center}
\end{figure}




